---
title: MindFrame
subtitle: Plumbing to use LLMs for psychologically informed interventions.
format: revealjs
author: Ben Whalley
date: 15 Oct 2024

---



## `mindframe`: Architecture

Goal: Flexible and scalable chatbot framework to deliver psychologically-informed interventions and *facilitate research*.



## History

- 1960s: Eliza (dialog via pattern matching and substitution)

- 1970s: Tree based 'expert systems' (structured inputs)

- 1980/90s: Knowledge representation (PROLOG), Bayesian networks (DAGs; PathFinder)

- <2017: Sentiment analysis, ML, NLP etc to inform rules-based systems


<!-- prolog example:

parent(X, Y) :- father(X, Y).
father(john, mary).
Query: parent(john, mary).



DAG - Baysian network example:

P(Fever | Flu) = 0.8
P(No Fever | Flu) = 0.2


 PathFinder
Domain: Medical Pathology
Description:
Developed to assist pathologists in diagnosing lymph-node diseases.
One of the earliest expert systems to use Bayesian networks for medical decision-making.
How It Worked:
Nodes in the DAG represented diseases, symptoms, and test results.
Conditional probabilities were assigned to the edges to capture relationships between diseases and symptoms.
The system computed the likelihood of various diagnoses based on the input evidence (e.g., symptoms, lab test results).
Key Features:
Explained its reasoning process by showing probabilistic dependencies.
Suggested additional tests to reduce diagnostic uncertainty.

-->


## Commercial ML-therapists

Woebot 2017 (CBT, initially rule based, now LLM)

Wysa  (CBT, DBT, IAPT waitlist)

Replika (commercial AI 'friend')

Flow/control structures not documented (?)


## Related

- Interactive Voice Response telephony (VoiceXML, Twilio)
- Workflow/business process automation (airflow, luigi)
- Chatbot frameworks (Rasa, Botpress, Dialogflow)


## Google Dialogflow

![](google-cx-page.svg)


## Open source

- `EmoCareAI/ChatPsychiatrist` (llama fine tune)
- `avocadopelvis/pandora` (gpt3 fine tune)
- `scofield7419/EmpathyEar` (multimodal fine-tune)
- amrita-thakur/mental-healthcare-chatbot (RAG based)
- `nmichaud0/TherapistGPT` (django app, prompt-based, hard-coded, incomplete/not working/abandoned)




## Approaches

- Tree-based systems/Flows/DAGs (structured inputs)
- Fine-tuning/multimodal models (therapy-like)
- Exemplar based (semantic search/RAG)
- LLM base-models + prompt-engineering 


<!---

## Key Features

  - Coordinate multiple models and track state/provide persistence
  - Modular design: break complex interventions into manageable components.
  - Monitoring, evaluation, iterative improvement
  - Scaffolding for **human supervision**, **intervention tracking**, and **testing**.
  - Provides structured data for research purposes.

## Roles

Different UI/UX to support each type of user

  - Client (chatbot)
  - Supervisor (web/mobile)
  - Intervention developer (text DSL; web)
  - Clinical manager (web)
  - System administrator (web; logs)


---

-->



## Overview

The biggest challenges are not technical: collaboration, introspection, validation

We need *constructs* to collaborate and *tools* to orchestrate.


<!--

## Therapy is a state-dependent directed graph

Therapy is a directed graph of plans and state-dependent adaptation.

State at each path is a path-dependent function:

$f(v_i) = g(v_0, v_1, \dots, v_{i-1})$

$g$ aggregates the history of the interactions to date.

- Each node has it's own function $g$
- Each node is an LLM prompt + deterministic function calling
- We 'cycle' on each node for some time, until a transition occurs
- 'Pacing' the therapy means managing these cycles and explicitly defining  transition rules




## ...

![](mermaid-diagram-2024-10-14-160723.png)


-->



## Core elements

![](steps.png)

<!--

## Non-Linear progress

**Directed Graph**: multiple possible paths.

- Implications of cycles?
- How do we ensure completion?

-->


## Templates & prompt engineering

Requirements:

- Collaboration
- Explicitness
- Modularity

Trade-off between

- Dynamism
- Simple/predictable


## Markdown

Lightweight markup designed for human readability and structural simplicity.

- common language for steps, judgements
- easy to read and write
- machine-readable/parsable
- extensible


## Templates



```

-----------

You are a therapist working with {client_name}
This is the conversations so far:

{{turns}}

Your task: 

- Help them work on  {{desired_change}}
- Ask them to think about barriers to {{desired_change}}

[[RESPONSE]]
```


## Exemplars

```

-----------

[...]

Previously, in cases like this, therapists said things like:

{% examples "therapists asking clients about {desired_change}" %}

Use the examples to inform what you say next:

[[RESPONSE]]
```



## Multistep promoting

```

-----------

You are a therapist treating a patient for anxiety.
They describe feeling overwhelmed by their workload.

These are the last few things the client said:

CLIENT: "I have so much to do, and it feels like I’ll 
never get through it. I don’t even know where to start."

Before responding, think step by step about the situation:

1. What is the client feeling?
2. How does this relate to their anxiety?
3. What might be a helpful therapeutic response?

[[DISCUSSION]]

```



## Multistep prompting (2)


```

-----------

[...]
3. What might be a helpful therapeutic response?

[[DISCUSSION]]

Based on your reasoning, craft a response that
validates their feelings and helps them move forward.
Be concise and respond in 1 or MAX 2 sentences.

[[UTTERANCE]]
```


## Judgements (templated LLM calls)

```

-----------

This is the conversation:

{{turns}}

Has the client described a behaviour they would like to change?
Respond using structured data, using these options only:

- No=no
- Briefly=brief
- At length=full
```


## Magentic constrains classification

<br>

```python
class ClinicalJudgement(BaseModel):
    response: Literal["no", "brief", "full"]
    explanation: str
```


## Transitions

Python expressions based on stored judgements:

```

-----------

welcome_done is True
emotional_state == "neutral"
n_turns_step > 5
```

## Template syntax

- `{meta:client_name}`: Client metadata
- `{turns:10}`: Conversation history
- `{turns "QUERY"}`: Semantically related conversation history
- `{notes:VAR_NAME}`: Retrieves Judgements
- `{examples:"QUERY"}`: Searches for and inserts examples based on specified topics.



## Output filters

> "It sounds like you're feeling really overwhelmed by everything you have on your plate right now, and it's completely understandable to feel that way when things seem so big and unmanageable. It’s okay to not know where to start sometimes. How about we break it down together? Maybe we can pick one small thing to focus on—something that feels a bit more doable. Taking things step by step might help you regain a sense of control, and I’ll be here to support you through it."



## Output filters (prompt)

```

-----------

{{TEXT}}

This is something an AI therapist said.
It is too complex and has too many parts.
No real therapist would say this much.

Make this more realistic.
Make it fit the dialogue.
Avoid repetition.
Ideally only 1 sentence response 
(we can say more later)
```


## Output filters (result)

> "Sounds like you’ve got a lot going on, and that’s totally understandable. How about we focus on just one thing to start with? We’ll take it step by step and see how it goes."




## Clinical notes

```

-----------

You are doing MI with a client
We have just completed the "Elicit current/future discrepancy" step.
We need to keep good notes so we can remember later.
This is the conversation:

{turns:step:previous}
{turns:step:current}

Summarise this conversations. 
Make a note which will be useful later.
Focus on: 
- what the client said and 
- what the client's goals are.

Respond in < 3 paragraphs.
[[NOTE]]

Respond in a single line
[[SUMMARY]]
```



## Fake clients

![](fakeclient.png)



## Synthetic data

![](synthetic.png)




<!--

## Cost implications

Optimise for quality first (token budget not important)

Latency more of an issue but:

- hardware getting faster (100s tokens/s)
- parallelise 'thinking' and 'speaking'?



## Costs

Assume

- 150 words per minute
- Input tokens are 6x output tokens
- 20x as many 'hidden' tokens as visible tokens
- Current OpenAI GPT4 costs (\$\$\$)

**Worst case: $4.73 per hour**

<!-- https://chatgpt.com/share/670e2135-1748-8001-a846-d1a470c0f5ef -->




## Roadmap

- Pilot system
- Work with Tx developers to build prompts
- Evaluate synthetic and real data
- Iterate templates

- Fine tune open models?
- Performance tuning?

- Factorial intervention evaluation





`